{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6486c8-9f66-4bfb-bbf2-8a090a8b1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has all the sentiment functionality. If reading the scraped data from the csv file, use get_all_sentiments_csv else to scrape and generate sentiments, use get_all_sentiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387c4f64-bf1e-40db-9926-d09000891c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-06 01:31:35,392 loading file C:\\Users\\sjufa\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "# Import al lthe supporting functions from sentiment_functions.py\n",
    "from sentiment_functions import score_flair, get_news, get_gnews_article, get_sentiments, getTwitterData,get_flair_score, get_news_flair_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943ea62-4ce7-4668-8345-7cd661759c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d531be-b246-47d5-ac8b-cd61fd1007a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item</th>\n",
       "      <th>POS</th>\n",
       "      <th>Aff_Score</th>\n",
       "      <th>Neg_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'em</td>\n",
       "      <td>PR</td>\n",
       "      <td>0.379542</td>\n",
       "      <td>0.533419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'n</td>\n",
       "      <td>CC</td>\n",
       "      <td>1.413176</td>\n",
       "      <td>1.199917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'n handle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.839000</td>\n",
       "      <td>2.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'s a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.023077</td>\n",
       "      <td>-0.013615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'s abc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.261667</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20545</th>\n",
       "      <td>zoom</td>\n",
       "      <td>VB</td>\n",
       "      <td>-0.652273</td>\n",
       "      <td>-0.649174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20546</th>\n",
       "      <td>zortrades.com</td>\n",
       "      <td>NN</td>\n",
       "      <td>2.141000</td>\n",
       "      <td>2.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20547</th>\n",
       "      <td>zte</td>\n",
       "      <td>NN</td>\n",
       "      <td>4.934000</td>\n",
       "      <td>5.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20548</th>\n",
       "      <td>zuck</td>\n",
       "      <td>NN</td>\n",
       "      <td>-0.237857</td>\n",
       "      <td>-0.185172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20549</th>\n",
       "      <td>zuckerberg</td>\n",
       "      <td>NN</td>\n",
       "      <td>-0.309667</td>\n",
       "      <td>-0.236923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20550 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Item  POS  Aff_Score  Neg_Score\n",
       "0                'em   PR   0.379542   0.533419\n",
       "1                 'n   CC   1.413176   1.199917\n",
       "2          'n handle  NaN   2.839000   2.941000\n",
       "3               's a  NaN  -0.023077  -0.013615\n",
       "4             's abc  NaN   2.261667   2.300000\n",
       "...              ...  ...        ...        ...\n",
       "20545           zoom   VB  -0.652273  -0.649174\n",
       "20546  zortrades.com   NN   2.141000   2.163000\n",
       "20547            zte   NN   4.934000   5.084000\n",
       "20548           zuck   NN  -0.237857  -0.185172\n",
       "20549     zuckerberg   NN  -0.309667  -0.236923\n",
       "\n",
       "[20550 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(Path('./Resources/lexicon_data/stock_lex.csv'))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8683935-ac15-477c-bbe5-a815f55bd6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## Scrape the web for Google news headlies and tweets for a 'key' for 'no_days' days and generate the corresponding sentiment scores\n",
    "def get_all_sentiments(key, no_days):\n",
    "    \n",
    "    # A list of dates from today going back 'no_days' in ascending order\n",
    "    datelist = pd.bdate_range(end = dt.date(2021, 9, 3), periods = int(no_days))\n",
    "\n",
    "    # Google News Analysis\n",
    "    \n",
    "    summary_df = []\n",
    "\n",
    "    # iterate through the list of dates to get the headlines for each date\n",
    "    for date in datelist:\n",
    "        df = pd.DataFrame(get_news(key, date)) # get_news function reads the RSS feed for google news for the specified date and extracts all the headlines for that date\n",
    "        \n",
    "        # We just want to analyze the top 20 headlines for a particular date\n",
    "        if(len(df) > 20):\n",
    "            article_df = get_gnews_article(df[:20]) # Using the newspaper package, extract the article, its summary and the keywords   \n",
    "        else:\n",
    "            article_df = get_gnews_article(df)\n",
    "        \n",
    "        #append the extracted article to a list\n",
    "        \n",
    "        summary_df.append(article_df) \n",
    "        \n",
    "    # To convert the list of articles, into a dateframe and set the index to the Date column\n",
    "    df2 = pd.concat(summary_df).set_index('Date')\n",
    "    df2 = df2.dropna()\n",
    "    df2 = df2.reset_index()\n",
    "    \n",
    "    # Group the articles by date - The resulting dataframe consists of all the extracted articles for a particular date, in seperate columns\n",
    "    dic = {}\n",
    "    for i in range(len(df2)):\n",
    "        date = df2['Date'][i]\n",
    "        summary = df2['Article'][i]\n",
    "        if date in dic:\n",
    "            dic[date].append(summary)\n",
    "        else:\n",
    "            dic[date] = [summary]\n",
    "\n",
    "    news_df = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "    news_df = news_df.reset_index()\n",
    "    news_df['index'] = pd.to_datetime(news_df['index'])\n",
    "    \n",
    "    news_df = news_df.replace(np.nan, 'none')\n",
    "    news_df = news_df.replace('', 'none')\n",
    "\n",
    "    # Get sentiment analysis score using vader and textblob for all the articles, agreggated and grouped by the date\n",
    "    news_sentiment = get_sentiments(news_df)\n",
    "    \n",
    "    # Get the flair analysis score for all the articles agreggated and grouped by date\n",
    "    news_flair_list = get_news_flair_score(news_df)\n",
    "    \n",
    "    # concat the sentiments in a new dataframe\n",
    "    news_sentiment['scores_flair'] = news_flair_list\n",
    "    \n",
    "    \n",
    "    # Twitter Analysis\n",
    "    \n",
    "    # Get tweets for the hashtag for no_days days\n",
    "    tweets_df = getTwitterData(key, no_days) \n",
    "    tweets_df = tweets_df.reset_index()\n",
    "    \n",
    "    # Convert the tweets dataframe into a new dataframe grouped by date where all the tweets for a particular date and in seperate columns\n",
    "    tweet_dic = {}\n",
    "    for i in range(len(tweets_df)):\n",
    "        date = tweets_df['Date'][i]\n",
    "        summary = tweets_df['full_text'][i]\n",
    "        if date in tweet_dic:\n",
    "            tweet_dic[date].append(summary)\n",
    "        else:\n",
    "            tweet_dic[date] = [summary]\n",
    "        \n",
    "    twitter_df = pd.DataFrame.from_dict(tweet_dic, orient = 'index')\n",
    "    twitter_df = twitter_df.reset_index()\n",
    "    twitter_df['index'] = pd.to_datetime(twitter_df['index'])\n",
    "    \n",
    "    twitter_df = twitter_df.replace(np.nan, 'none')\n",
    "    twitter_df = twitter_df.replace('', 'none')\n",
    "    \n",
    "    # Perform Vader and Textblob sentiment analysis on the tweets    \n",
    "    tweets_sentiment = get_sentiments(twitter_df)    \n",
    "    tweets_sentiment.columns = ['index', 'tw_subj_score', 'tw_simi_score', 'tw_vader_score']\n",
    "    \n",
    "    # Perform flair analysis on the tweets\n",
    "    flair_list = get_flair_score(twitter_df)\n",
    "    \n",
    "    # Concat all the sentiments\n",
    "    tweets_sentiment['tw_scores_flair'] = flair_list\n",
    "    \n",
    "    # Join the google news and twitter sentiments in to new dataframe\n",
    "    sentiment_df = news_sentiment.join(tweets_sentiment.set_index('index'), on = 'index')\n",
    "    sentiment_df = sentiment_df.dropna()\n",
    "    \n",
    "    # return the generated dataframe with the required signals\n",
    "    return sentiment_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7092ebd-985a-4d34-a694-ae6e7f50aff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Read the already scrapped google news articles and tweets that are stored in csv files to generate the sentiments for a 'key' for 'no_days' days\n",
    "\n",
    "def get_all_sentiments_csv(key, no_days):\n",
    "    \n",
    "    # A list of dates from today going back 'no_days' in ascending order\n",
    "    datelist = pd.bdate_range(end = dt.date(2021, 9, 3), periods = int(no_days))\n",
    "    \n",
    "    # Convert the key to uppercase to match the csv file naming convention\n",
    "    if key.isupper() == False:\n",
    "        key = key.upper()\n",
    "\n",
    "    # Define a path to read the file\n",
    "    csvpath = Path(\"Resources/news_data/{}_200.csv\".format(key))\n",
    "    \n",
    "    #read the file \n",
    "    df2 = pd.read_csv(csvpath)\n",
    "    # df2 = df2.drop(columns = ['Unnamed: 0'])\n",
    " \n",
    "    # Group the read dataframe by date so that each row will contain all the articles for a particular date in different columns\n",
    "    dic = {}\n",
    "    for i in range(len(df2)):\n",
    "        date = df2['Date'][i]\n",
    "        summary = df2['Article'][i]\n",
    "        if date in dic:\n",
    "            dic[date].append(summary)\n",
    "        else:\n",
    "            dic[date] = [summary]\n",
    "\n",
    "    news_df = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "    news_df = news_df.reset_index()\n",
    "    news_df['index'] = pd.to_datetime(news_df['index'])\n",
    "    \n",
    "    news_df = news_df.replace(np.nan, 'none')\n",
    "    news_df = news_df.replace('', 'none')\n",
    "\n",
    "    \n",
    "    # Get the vader and textblob sentiments using the entire article\n",
    "    news_sentiment = get_sentiments(news_df)\n",
    "    \n",
    "    # To get the flair score we are just using the summary of the article, so create another dataframe whose rows contain the summary of the articles for a particular date in its columns\n",
    "    dic2 = {}\n",
    "    for i in range(len(df2)):\n",
    "        date = df2['Date'][i]\n",
    "        summary = df2['Summary'][i]\n",
    "        if date in dic:\n",
    "            dic[date].append(summary)\n",
    "        else:\n",
    "            dic[date] = [summary]\n",
    "\n",
    "    news_summary_df = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "    news_summary_df = news_summary_df.reset_index()\n",
    "    news_summary_df['index'] = pd.to_datetime(news_summary_df['index'])\n",
    "    \n",
    "    news_summary_df = news_summary_df.replace(np.nan, 'none')\n",
    "    news_summary_df = news_summary_df.replace('', 'none')\n",
    "    \n",
    "    # Get flair scores for the news articcle summary\n",
    "    news_flair_list = get_news_flair_score(news_summary_df)\n",
    "    news_sentiment['scores_flair'] = news_flair_list\n",
    "    \n",
    "      \n",
    "    # Read the csv file containg the tweets\n",
    "    csvpath2 = Path(\"Resources/news_data/{}_twt_200.csv\".format(key))\n",
    "    tweets_df = pd.read_csv(csvpath2)\n",
    "    \n",
    "    tweet_dic = {}\n",
    "    for i in range(len(tweets_df)):\n",
    "        date = tweets_df['Date'][i]\n",
    "        summary = tweets_df['full_text'][i]\n",
    "        if date in tweet_dic:\n",
    "            tweet_dic[date].append(summary)\n",
    "        else:\n",
    "            tweet_dic[date] = [summary]\n",
    "        \n",
    "    twitter_df = pd.DataFrame.from_dict(tweet_dic, orient = 'index')\n",
    "    twitter_df = twitter_df.reset_index()\n",
    "    twitter_df['index'] = pd.to_datetime(twitter_df['index'])\n",
    "    \n",
    "    twitter_df = twitter_df.replace(np.nan, 'none')\n",
    "    twitter_df = twitter_df.replace('', 'none')\n",
    "    \n",
    "    \n",
    "    # Get the sentiment data - vader and textblob score for Tweets\n",
    "    \n",
    "    tweets_sentiment = get_sentiments(twitter_df)    \n",
    "    tweets_sentiment.columns = ['index', 'tw_subj_score', 'tw_simi_score', 'tw_vader_score']\n",
    "    \n",
    "    # Get the flair score for the tweets\n",
    "    flair_list = get_flair_score(twitter_df)\n",
    "    tweets_sentiment['tw_scores_flair'] = flair_list\n",
    "    \n",
    "    # Join the two sentiment blocks and return the final sentiment data dataframe.\n",
    "    sentiment_df = news_sentiment.join(tweets_sentiment.set_index('index'), on = 'index')\n",
    "    sentiment_df = sentiment_df.dropna()\n",
    "    \n",
    "    return sentiment_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe0a94f-fbcf-4976-962c-346ffcbf4b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\abhis\\OneDrive\\Desktop\\Fintech-Workspace\\Project_2\\sentiment_functions.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_sentiment_df['subj_score'] = news_weighted_subj_senti_df.mean(axis = 1, skipna = True)\n",
      "C:\\Users\\abhis\\OneDrive\\Desktop\\Fintech-Workspace\\Project_2\\sentiment_functions.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_sentiment_df['simi_score'] = news_weighted_simi_senti_df.mean(axis = 1, skipna = True)\n",
      "C:\\Users\\abhis\\OneDrive\\Desktop\\Fintech-Workspace\\Project_2\\sentiment_functions.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_sentiment_df['vader_score'] = news_vader_compound_df.mean(axis = 1, skipna = True)\n",
      "C:\\Users\\abhis\\Anaconda3\\envs\\project2-dev\\lib\\site-packages\\ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\envs\\project2-dev\\lib\\site-packages\\ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subj_score</th>\n",
       "      <th>simi_score</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>scores_flair</th>\n",
       "      <th>tw_subj_score</th>\n",
       "      <th>tw_simi_score</th>\n",
       "      <th>tw_vader_score</th>\n",
       "      <th>tw_scores_flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>0.155822</td>\n",
       "      <td>0.033062</td>\n",
       "      <td>0.428187</td>\n",
       "      <td>0.763038</td>\n",
       "      <td>0.109633</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.251724</td>\n",
       "      <td>0.852676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>0.079965</td>\n",
       "      <td>-0.092097</td>\n",
       "      <td>0.148340</td>\n",
       "      <td>0.698710</td>\n",
       "      <td>0.107722</td>\n",
       "      <td>0.176904</td>\n",
       "      <td>0.358940</td>\n",
       "      <td>0.903324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-02</td>\n",
       "      <td>0.106608</td>\n",
       "      <td>-0.057984</td>\n",
       "      <td>0.247887</td>\n",
       "      <td>0.711171</td>\n",
       "      <td>0.053968</td>\n",
       "      <td>0.079193</td>\n",
       "      <td>0.136816</td>\n",
       "      <td>0.844240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>0.064682</td>\n",
       "      <td>-0.085852</td>\n",
       "      <td>0.156960</td>\n",
       "      <td>0.724906</td>\n",
       "      <td>0.143816</td>\n",
       "      <td>0.306866</td>\n",
       "      <td>0.320792</td>\n",
       "      <td>0.834476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-04</td>\n",
       "      <td>0.118216</td>\n",
       "      <td>-0.116589</td>\n",
       "      <td>0.241673</td>\n",
       "      <td>0.707307</td>\n",
       "      <td>0.049216</td>\n",
       "      <td>-0.048452</td>\n",
       "      <td>0.088508</td>\n",
       "      <td>0.761727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2021-08-27</td>\n",
       "      <td>0.414973</td>\n",
       "      <td>1.012966</td>\n",
       "      <td>0.958113</td>\n",
       "      <td>0.946642</td>\n",
       "      <td>0.071646</td>\n",
       "      <td>-0.026065</td>\n",
       "      <td>0.169872</td>\n",
       "      <td>0.729967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2021-08-30</td>\n",
       "      <td>0.342376</td>\n",
       "      <td>1.026747</td>\n",
       "      <td>0.930040</td>\n",
       "      <td>0.932698</td>\n",
       "      <td>0.070490</td>\n",
       "      <td>0.165825</td>\n",
       "      <td>0.275548</td>\n",
       "      <td>0.847707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>0.430080</td>\n",
       "      <td>0.977830</td>\n",
       "      <td>0.964060</td>\n",
       "      <td>0.964923</td>\n",
       "      <td>0.148670</td>\n",
       "      <td>0.220250</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.868084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>0.396297</td>\n",
       "      <td>0.840783</td>\n",
       "      <td>0.925427</td>\n",
       "      <td>0.934073</td>\n",
       "      <td>0.149433</td>\n",
       "      <td>0.216940</td>\n",
       "      <td>0.347460</td>\n",
       "      <td>0.878803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2021-09-02</td>\n",
       "      <td>0.351173</td>\n",
       "      <td>0.755989</td>\n",
       "      <td>0.853227</td>\n",
       "      <td>0.946024</td>\n",
       "      <td>0.114470</td>\n",
       "      <td>0.207943</td>\n",
       "      <td>0.276008</td>\n",
       "      <td>0.888549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  subj_score  simi_score  vader_score  scores_flair  \\\n",
       "0   2020-11-30    0.155822    0.033062     0.428187      0.763038   \n",
       "1   2020-12-01    0.079965   -0.092097     0.148340      0.698710   \n",
       "2   2020-12-02    0.106608   -0.057984     0.247887      0.711171   \n",
       "3   2020-12-03    0.064682   -0.085852     0.156960      0.724906   \n",
       "4   2020-12-04    0.118216   -0.116589     0.241673      0.707307   \n",
       "..         ...         ...         ...          ...           ...   \n",
       "193 2021-08-27    0.414973    1.012966     0.958113      0.946642   \n",
       "194 2021-08-30    0.342376    1.026747     0.930040      0.932698   \n",
       "195 2021-08-31    0.430080    0.977830     0.964060      0.964923   \n",
       "196 2021-09-01    0.396297    0.840783     0.925427      0.934073   \n",
       "197 2021-09-02    0.351173    0.755989     0.853227      0.946024   \n",
       "\n",
       "     tw_subj_score  tw_simi_score  tw_vader_score  tw_scores_flair  \n",
       "0         0.109633       0.178726        0.251724         0.852676  \n",
       "1         0.107722       0.176904        0.358940         0.903324  \n",
       "2         0.053968       0.079193        0.136816         0.844240  \n",
       "3         0.143816       0.306866        0.320792         0.834476  \n",
       "4         0.049216      -0.048452        0.088508         0.761727  \n",
       "..             ...            ...             ...              ...  \n",
       "193       0.071646      -0.026065        0.169872         0.729967  \n",
       "194       0.070490       0.165825        0.275548         0.847707  \n",
       "195       0.148670       0.220250        0.320300         0.868084  \n",
       "196       0.149433       0.216940        0.347460         0.878803  \n",
       "197       0.114470       0.207943        0.276008         0.888549  \n",
       "\n",
       "[198 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Example function impementation fro generating the data for a 100 days for the stocks listed in the list.\n",
    "\n",
    "list = ['AAPL', 'BAC','CRM','GOOG','INTC', 'MSFT','NVDA','PYPL','TSLA']\n",
    "\n",
    "for i in range(len(list)):\n",
    "    s_df = get_all_sentiments_csv(list[i],100)\n",
    "    s_df.to_csv('Resources/Combined Sentiment signals/{}.csv'.format(list(i)))\n",
    "    s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b72dd5-3f57-45ae-a335-bdeb5a31d8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
